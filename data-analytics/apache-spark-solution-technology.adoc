---
sidebar: sidebar 
permalink: data-analytics/apache-spark-solution-technology.html 
keywords: standalone, apache mesos, hadoop yarn, resilient distributed dataset, rdd, dataframe, hadoop distributed file system, hdfs 
summary: 'Cette section décrit la nature et les composants d"Apache Spark et leur contribution à cette solution.' 
---
= Technologie de la solution
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:apache-spark-target-audience.html["Précédent : audience cible."]

[role="lead"]
Apache Spark est un framework de programmation célèbre qui permet de rédiger des applications Hadoop directement avec le système Hadoop Distributed File System (HDFS). Spark est prête pour la production et prend en charge le traitement du streaming de données, et elle est plus rapide que MapReduce. Spark propose une mise en cache des données en mémoire configurable pour une itération efficace, et le shell Spark est interactif pour l'apprentissage et l'exploration des données. Avec Spark, vous pouvez créer des applications en Python, Scala ou Java. Les applications SPARK consistent en un ou plusieurs travaux qui ont une ou plusieurs tâches.

Chaque application Spark est dotée d'un tournevis à bougie. En mode YARN-client, le pilote s'exécute localement sur le client. En mode YARN-Cluster, le pilote s'exécute dans le cluster du maître d'application. En mode cluster, l'application continue à fonctionner même si le client se déconnecte.

image:apache-spark-image3.png["Erreur : image graphique manquante"]

Il existe trois gestionnaires de cluster :

* *Autonome.* ce gestionnaire fait partie de Spark, ce qui facilite l'installation d'un cluster.
* *Apache Mesos.* il s'agit d'un gestionnaire de cluster général qui exécute également MapReduce et d'autres applications.
* *HADOOP YARN.* il s'agit d'un gestionnaire de ressources dans Hadoop 3.


Le jeu de données distribué résilient (RDD) est le composant principal de Spark. RDD recrée les données perdues et manquantes des données stockées dans la mémoire du cluster et stocke les données initiales provenant d'un fichier ou créées par programmation. Les RDD sont créés à partir de fichiers, de données en mémoire ou d'un autre RDD. La programmation des étincelles effectue deux opérations : la transformation et les actions. Transformation crée un nouveau RDD basé sur un RDD existant. Les actions renvoient une valeur à partir d'un RDD.

Les transformations et les actions s'appliquent également aux ensembles de données Spark et aux DataFrames. Un jeu de données est un ensemble distribué de données qui offre les avantages des RDD (fort typage, utilisation des fonctions lambda) avec les avantages du moteur d'exécution optimisé de Spark SQL. Un jeu de données peut être construit à partir d'objets JVM, puis manipulé à l'aide de transformations fonctionnelles (MAP, plantMap, filtre, etc.). Un DataFrame est un jeu de données organisé en colonnes nommées. Il est conceptuellement équivalent à une table dans une base de données relationnelle ou à une trame de données dans R/Python. DataFrames peut être construit à partir d'un large éventail de sources, telles que des fichiers de données structurés, des tables dans Hive/HBase, des bases de données externes sur site ou dans le cloud, ou des disques durs virtuels existants.

Les applications Spark incluent un ou plusieurs travaux Spark. Les travaux exécutent des tâches dans des exécuteurs et les exécuteurs exécutent des conteneurs DE FILS. Chaque exécuteur s’exécute dans un seul conteneur et des exécuteurs existent tout au long de la durée de vie d’une application. Un exécuteur est corrigé après le démarrage de l'application et LE FIL ne redimensionne pas le conteneur déjà alloué. Un exécuteur peut exécuter des tâches simultanément sur des données en mémoire.

link:apache-spark-netapp-spark-solutions-overview.html["Ensuite : présentation des solutions NetApp Spark"]
