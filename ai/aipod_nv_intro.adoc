---
sidebar: sidebar 
permalink: ai/aipod_nv_intro.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: 'NetApp AIPod avec les systèmes NVIDIA DGX est une architecture de référence professionnelle basée sur NVIDIA BasePOD pour le deep learning et l"intelligence artificielle grâce aux systèmes de stockage NetApp ONTAP AFF, à la mise en réseau NVIDIA et aux systèmes DGX.' 
---
= NetApp AIPod avec les systèmes NVIDIA DGX - Présentation
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
Cette section présente NetApp AIPod avec des systèmes NVIDIA DGX.

Ingénierie de solutions NetApp

NetApp&#8482 ; AIPod avec NVIDIA DGX&#8482 ; les systèmes et les systèmes de stockage NetApp connectés au cloud simplifient les déploiements d'infrastructure pour les workloads de machine learning (ML) et d'intelligence artificielle (IA) en éliminant la complexité de la conception et les approximations. Reposant sur la conception NVIDIA DGX BasePOD pour offrir des performances de calcul exceptionnelles pour les charges de travail nouvelle génération, AIPod avec les systèmes NVIDIA DGX ajoute des systèmes de stockage NetApp AFF qui permettent aux clients de commencer avec un déploiement de petite taille, puis d'évoluer de manière non disruptive tout en gérant intelligemment les données de la périphérie au cœur, et jusqu'au cloud, et inversement. NetApp AIPod fait partie du portefeuille plus vaste de solutions d'IA de NetApp, illustré dans la figure ci-dessous :

_Portefeuille de solutions NetApp pour l'IA_
image:aipod_nv_portfolio.png["Erreur : image graphique manquante"]

Ce document décrit les principaux composants de l'architecture de référence AIPod, les informations sur la connectivité du système et les conseils sur le dimensionnement de la solution. Ce document est destiné aux ingénieurs de solutions partenaires et NetApp, ainsi qu'aux décideurs stratégiques des clients intéressés par le déploiement d'une infrastructure haute performance pour les workloads de ML/DL et d'analytique.
