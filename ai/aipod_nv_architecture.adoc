---
sidebar: sidebar 
permalink: ai/aipod_nv_architecture.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: FlexPod pour IA NetApp avec les systèmes NVIDIA DGX - Architecture 
---
= FlexPod pour IA NetApp avec les systèmes NVIDIA DGX - Architecture
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:aipod_nv_sw_components.html["Précédent : ONTAP ai - composants logiciels."]

Cette architecture de référence exploite des structures distinctes pour l'interconnexion des clusters de calcul et l'accès au stockage, avec des options de connectivité InfiniBand (IB) NDR200 et HDR200 entre les nœuds de calcul. Les systèmes DGX H100 sont livrés avec des cartes ConnectX-7 préinstallées pour la connectivité NDR IB, tandis que les systèmes DGX A100 peuvent utiliser respectivement des cartes ConnectX-6 ou ConnectX-7 pour la connectivité HDR ou NDR.



== ONTAP ai avec les systèmes DGX H100

La figure ci-dessous présente la topologie globale de la solution lors de l'utilisation de systèmes DGX H100 avec ONTAP ai.

image:oai_H100_topo.png["Erreur : image graphique manquante"]

Dans cette configuration, le réseau du cluster de calcul utilise une paire de commutateurs IB QM9700 NDR, qui sont connectés ensemble pour une haute disponibilité. Chaque système DGX H100 est connecté aux switchs par huit connexions NDR200, avec des ports paires connectés à un switch et des ports impaires connectés à l'autre switch.

Pour l'accès au système de stockage, la gestion intrabande et l'accès client, une paire de commutateurs Ethernet SN4600 est utilisée. Les commutateurs sont connectés avec des liaisons inter-commutateurs et configurés avec plusieurs VLAN pour isoler les différents types de trafic. Pour les déploiements de plus grande envergure, le réseau ethernet peut être étendu à une configuration Leaf-Spine en ajoutant des paires de switchs supplémentaires pour une spine et des lames supplémentaires, si nécessaire. Chaque système DGX A100 est doté de deux cartes ConnectX-6 à deux ports pour le trafic ethernet et de stockage. Pour cette solution, les quatre ports sont connectés aux switchs Ethernet SN4600 à 200 Gbit/s. Un port de chaque carte est configuré en une liaison LACP MLAG avec un port connecté à chaque switch, et les VLAN pour la gestion intrabande, l'accès client et l'accès au stockage au niveau utilisateur sont hébergés sur cette liaison. L'autre port de chaque carte est utilisé de façon indépendante dans des VLAN de stockage RoCE dédiés distincts pour la connectivité au système de stockage AFF A800. Ces ports prennent en charge l'accès au stockage haute performance via NFS v3, NFSv4.x avec pNFS et NFS sur RDMA.

Outre l'interconnexion de calcul et les réseaux ethernet haut débit, tous les périphériques physiques sont également connectés à un ou plusieurs commutateurs Ethernet SN2201 pour la gestion hors bande.  Pour en savoir plus sur la connectivité des systèmes DGX A100, consultez le link:https://nvdam.widen.net/s/nfnjflmzlj/nvidia-dgx-basepod-reference-architecture["Documentation NVIDIA BasePOD"].



== ONTAP ai avec les systèmes DGX A100

La figure ci-dessous présente la topologie globale de la solution lors de l'utilisation de systèmes DGX A100 et d'une structure de calcul HDR avec ONTAP ai.

image:oai_A100_topo.png["Erreur : image graphique manquante"]

Dans cette configuration, le réseau du cluster de calcul utilise une paire de commutateurs QM8700 HDR IB, connectés ensemble pour une haute disponibilité. Chaque système DGX A100 est connecté aux switchs par quatre cartes ConnectX-6 à un port à 200 Gbit/s, avec des ports paires connectés à un switch et des ports impaires connectés à l'autre switch.

Pour l'accès au système de stockage, la gestion intrabande et l'accès client, une paire de commutateurs Ethernet SN4600 est utilisée. Les commutateurs sont connectés avec des liaisons inter-commutateurs et configurés avec plusieurs VLAN pour isoler les différents types de trafic. Pour les déploiements de plus grande envergure, le réseau ethernet peut être étendu à une configuration Leaf-Spine en ajoutant des paires de switchs supplémentaires pour une spine et des lames supplémentaires, si nécessaire. Chaque système DGX A100 est doté de deux cartes ConnectX-6 à deux ports pour le trafic ethernet et de stockage. Pour cette solution, les quatre ports sont connectés aux switchs Ethernet SN4600 à 200 Gbit/s. Un port de chaque carte est configuré en une liaison LACP MLAG avec un port connecté à chaque switch, et les VLAN pour la gestion intrabande, l'accès client et l'accès au stockage au niveau utilisateur sont hébergés sur cette liaison. L'autre port de chaque carte est utilisé de façon indépendante dans des VLAN de stockage RoCE dédiés distincts pour la connectivité au système de stockage AFF A800. Ces ports prennent en charge l'accès au stockage haute performance via NFS v3, NFSv4.x avec pNFS et NFS sur RDMA.

Outre l'interconnexion de calcul et les réseaux ethernet haut débit, tous les périphériques physiques sont également connectés à un ou plusieurs commutateurs Ethernet SN2201 pour la gestion hors bande.  Pour en savoir plus sur la connectivité des systèmes DGX A100, consultez le link:https://nvdam.widen.net/s/nfnjflmzlj/nvidia-dgx-basepod-reference-architecture["Documentation NVIDIA BasePOD"].



== Serveurs de plan de gestion

Cette architecture de référence comprend également cinq serveurs basés sur processeurs pour l'utilisation d'un plan de gestion. Deux de ces systèmes sont utilisés comme nœuds principaux pour le gestionnaire de commandes de base pour le déploiement et la gestion du cluster. Les trois autres systèmes sont utilisés pour fournir des services de cluster supplémentaires tels que les nœuds maîtres Kubernetes ou les nœuds de connexion pour les déploiements utilisant Slurm pour la planification des tâches. Les déploiements qui utilisent Kubernetes peuvent exploiter le pilote NetApp Astra Trident CSI pour fournir un provisionnement automatisé et des services de données avec un stockage persistant pour la gestion des workloads d'IA et sur le système de stockage AFF A800.

Chaque serveur est connecté physiquement aux switchs IB et ethernet pour permettre le déploiement et la gestion du cluster, et configuré avec des montages NFS sur le système de stockage via la SVM de gestion pour le stockage des artéfacts de gestion de cluster, comme décrit précédemment.

link:aipod_nv_storage.html["Next : FlexPod NetApp ai Pod avec les systèmes NVIDIA DGX - conseils de conception et de dimensionnement des systèmes de stockage."]
