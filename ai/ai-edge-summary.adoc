---
sidebar: sidebar 
permalink: ai/ai-edge-summary.html 
keywords:  
summary:  
---
= Récapitulatif
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


Plusieurs scénarios d'applications émergents, tels que les systèmes d'assistance à la conduite automobile (ADAS), le concept d'industrie 4.0, les Smart cities et l'Internet des objets (IoT), requièrent le traitement des flux de données en continu avec une latence quasi nulle. Ce document présente une architecture de calcul et de stockage permettant de déployer l'inférence d'intelligence artificielle (IA) basée sur les processeurs graphiques sur des contrôleurs de stockage NetApp et des serveurs Lenovo ThinkSystem dans un environnement en périphérie qui répond à ces exigences. Ce document fournit également des données de performance pour le banc d'essai MLPerf Inférence standard, en évaluant diverses tâches d'inférence sur des serveurs de périphérie équipés de processeurs graphiques NVIDIA T4. Nous examinons les performances des scénarios d'inférence hors ligne, à flux unique et à flux multiples. Nous montrons également que l'architecture associée à un système de stockage en réseau partagé économique est hautement performante et qu'elle constitue un point central pour la gestion des données et des modèles pour les serveurs de périphérie multiples.
