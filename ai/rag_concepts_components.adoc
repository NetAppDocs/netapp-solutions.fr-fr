---
sidebar: sidebar 
permalink: ai/rag_concepts_components.html 
keywords: RAG, Retrieval Augmented Generation, NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NeMo, NIM, NIMS, Hybrid, Hybrid Cloud, Hybrid Multicloud, NetApp ONTAP, FlexCache, SnapMirror, BlueXP 
summary: 'RAG d"entreprise avec NetApp - concepts et composants' 
---
= Concepts et composants
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/




== IA générative

Les systèmes d'IA tels que l'IA générative sont appliqués au machine learning non supervisé ou autogéré à un grand dataset. Contrairement aux modèles de machine learning classiques qui font des prédictions sur un dataset spécifique, les modèles d'IA génératifs sont capables de générer du contenu nouveau, tel que du texte, du code, de l'image, de la vidéo ou de l'audio, en réponse aux invites de l'utilisateur. Pour cette raison, les capacités d'un système d'IA générative sont également classées en fonction de la modalité ou du type des données utilisées. Il peut s'agir d'un modèle unimodal ou multimodal. Un système unimodal ne prend qu'un seul type d'entrée (par ex. Texte uniquement ou image uniquement), alors qu'un système multimodal peut prendre plusieurs types d'entrée (par ex. texte, image et audio), comprendre et générer simultanément du contenu sur différentes modalités. En substance, l'IA générative révolutionne la façon dont les entreprises créent du contenu, génèrent de nouveaux concepts de conception et extraient la valeur des données existantes.



=== Modèles de langage large (LLMS)

Les LLMS sont des modèles de deep learning pré-entraînés sur de grands volumes de données, qui peuvent entre autres reconnaître et générer du texte. Les LLMS ont commencé comme un sous-ensemble de l'IA générative qui se concentre principalement sur le langage, cependant, de telles distinctions s'estompent lentement à mesure que les LLMS multimodaux continuent à émerger. Le transformateur sous-jacent dans un LLM, introduit une nouvelle architecture de réseau autre que RNN ou CNN. Il a un ensemble de réseaux neuronaux qui se composent d'un encodeur et d'un décodeur qui aide à extraire le sens d'une séquence de texte et à comprendre la relation entre les mots. Les LLMS peuvent répondre au langage humain naturel et utiliser l'analyse des données pour répondre à une question non structurée. Cependant, les LLMS ne peuvent être aussi fiables que les données qu'ils ingèrent, donc enclins aux hallucinations, découlant des défis que posent les déchets dans les poubelles. Si les LLMS sont alimentés avec de fausses informations, ils peuvent générer des sorties inexactes en réponse à des requêtes utilisateur simplement pour correspondre au descriptif qu'il est en train de construire. Nos recherches fondées sur des données probantes suggèrent que les ingénieurs en IA utilisent diverses méthodes pour contrer ces hallucinations, l'une par des voies de fait qui limitent les résultats inexacts, et l'autre par des ajustements et un transfert d'apprentissage avec des données de qualité qui sont contextuellement pertinentes, par le biais de techniques comme RAG.



=== Récupération génération augmentée (RAG)

Les LLMS sont entraînés sur de grands volumes de données, mais ils ne sont pas entraînés sur vos données. RAG résout ce problème en ajoutant vos données aux données auxquels les LLMS ont déjà accès. Les clients peuvent tirer parti de la puissance d'un LLM entraîné sur leurs données, récupérant ainsi les informations et les utilisant pour fournir des informations contextuelles aux utilisateurs de l'IA générative. RAG est une technique de machine learning, une approche architecturale qui peut aider à réduire les hallucinations et à améliorer l'efficacité et la fiabilité des LLM, accélérer le développement d'applications d'IA et augmenter l'expérience de recherche d'entreprise.



=== Ragas

Il existe des outils et des cadres existants qui vous aident à créer des pipelines RAG, mais il peut être difficile de les évaluer et de quantifier les performances de votre pipeline. C'est là que racas (évaluation RAG) entre en jeu. Ragas est un cadre qui vous aide à évaluer vos pipelines RAG. Ragas vise à créer une norme ouverte, fournissant aux développeurs les outils et les techniques nécessaires pour tirer parti de l'apprentissage continu dans leurs applications RAG. Pour plus d'informations, reportez-vous à la section https://docs.ragas.io/en/stable/getstarted/index.html["Commencez avec Ragas"^]



== Llama 3

Le Llama 3 de Meta, un modèle de transformateur décodeur uniquement, est un modèle de langage large (LLM) pré-formé ouvert et accessible. Entraîné sur plus de 15 000 milliards de milliards de milliards de données, Llama 3 change la donne dans le domaine de la compréhension du langage naturel (NLU). Il excelle dans la compréhension contextuelle et les tâches complexes comme la traduction et la génération de dialogue. Llama 3 est disponible en deux tailles : 8B pour un déploiement et un développement efficaces, et 70B pour les applications natives d'IA à grande échelle. Les clients peuvent déployer Llama 3 sur Google Cloud via Vertex ai, sur Azure via Azure ai Studio et sur AWS via Amazon Sagemaker.

Dans notre validation, nous avons déployé le modèle Llama de Meta avec des microservices NVIDIA Nemo™, dans une instance NVIDIA DGX dont le calcul est accéléré avec les processeurs graphiques NVIDIA A100, pour personnaliser et évaluer un cas d'usage d'IA génératif, tout en prenant en charge la génération RAG (récupération augmentée) dans les applications sur site.



== Structures Open Source

Les informations supplémentaires suivantes sur les technologies open source peuvent s'avérer utiles selon votre déploiement.



=== Chaîne de langue

LangChain est un framework d'intégration open source pour le développement d'applications optimisées par des modèles de langage large (LLMS). Les clients peuvent créer efficacement des applications RAG grâce à document Loader, VectorStores et divers autres packages, ce qui permet aux développeurs de créer des flux de travail complexes. Ils peuvent également inspecter, surveiller et évaluer des applications avec LangSmith pour optimiser et déployer en permanence n'importe quelle chaîne dans une API REST avec LangServe. LangChain encode les meilleures pratiques pour les applications RAG et fournit des interfaces standard pour divers composants nécessaires à la construction d'applications RAG.



=== LlamaIndex

LlamaIndex est un framework de données simple et flexible permettant de connecter des sources de données personnalisées à des applications basées sur un modèle de langage large (LLM). Il vous permet d'ingérer des données à partir d'API, de bases de données, de PDF, etc. Via des connecteurs de données flexibles. Les LLMS comme Llama 3 et GPT-4 sont pré-entraînés sur des jeux de données publics massifs, ce qui permet d'obtenir des capacités de traitement du langage naturel incroyables, prêtes à l'emploi. Cependant, leur utilité est limitée sans accès à vos propres données privées. LlamaIndex fournit des bibliothèques Python et dactylographiées extrêmement populaires et est le leader de l'industrie dans les techniques de génération de récupération augmentée (RAG).



== Microservices NVIDIA Nemo

NVIDIA Nemo est une plateforme de bout en bout permettant de créer et de personnaliser des modèles d'IA génératifs haute performance pouvant être déployés partout, dans le cloud et les data centers. Nemo propose des microservices qui simplifient le processus de développement et de déploiement de l'IA générative à grande échelle, ce qui permet aux entreprises de connecter les LLMS à leurs sources de données d'entreprise. À ce moment-là, les microservices Nemo sont disponibles via un programme d'accès en avant-première proposé par NVIDIA.



=== Microservices NVIDIA Nemo Inférence (NIMS)

NVIDIA NIMS, intégré à NVIDIA ai Enterprise, permet de rationaliser le développement d'applications d'entreprise optimisées par l'IA et le déploiement de modèles d'IA en production. NIMS est un microservice d'inférence conteneurisé, qui inclut des API standard, du code spécifique d'un domaine, des moteurs d'inférence optimisés et l'exécution d'entreprise.



=== NVIDIA Nemo Retriever

NVIDIA Nemo Retriever, le dernier service de la structure NVIDIA Nemo, optimise la partie d'intégration et de récupération de RAG afin d'offrir une plus grande précision et des réponses plus efficaces. NVIDIA Nemo Retriever est un service de récupération d'informations qui peut être déployé sur site ou dans le cloud. Il permet aux entreprises d'intégrer des fonctionnalités RAG haute performance à leurs applications d'IA de production personnalisées en toute sécurité et en toute simplicité.



== Opérateur NVIDIA Enterprise RAG LLM

L'opérateur NVIDIA Enterprise Retrieval Augmented Generation (RAG) large Language Model (LLM) permet d'utiliser les composants logiciels et les services nécessaires pour exécuter les pipelines RAG dans Kubernetes. Il permet un accès en avant-première à un opérateur qui gère le cycle de vie des composants clés des pipelines RAG, tels que NVIDIA Inférence Microservice et NVIDIA Nemo Retriever Embedding Microservice. Pour plus d'informations, reportez-vous à la section https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/index.html["Opérateur NVIDIA Enterprise RAG LLM"^]



== Base de données vectorielle



=== PostgreSQL : pgvector

Avec ses liaisons natives pour de nombreux algorithmes de ML classiques tels que XGBoost, l'apprentissage machine avec SQL n'est pas une nouveauté dans PostgreSQL. Dernièrement, avec la sortie de pgvector, une extension open source pour la recherche par similarité de vecteur, PostgreSQL a la capacité de stocker et de rechercher des embeddings générés par le ML, une fonctionnalité utile pour les cas d'utilisation et les applications d'IA qui utilisent les LLMS.

L'exemple de pipeline par défaut dans notre validation avec l'opérateur NVIDIA Enterprise RAG LLM démarre la base de données pgvector dans un pod. Le serveur de requête se connecte ensuite à la base de données pgvector pour stocker et récupérer les embeddings. L'application web et le serveur de requêtes chat bot communiquent avec les microservices et la base de données vectorielle pour répondre aux invites de l'utilisateur.



=== Milvus

Comme la base de données polyvalente Vector qui propose une API, à l'instar de MongoDB, Milvus se distingue par sa prise en charge d'une grande variété de types de données et par des fonctionnalités telles que la multi-virtualisation, ce qui en fait un choix populaire pour la science des données et le machine learning. Les services IT sont en mesure de stocker, d'indexer et de gérer plus d'un milliard de vecteurs d'intégration générés par les modèles de réseaux neuronaux profonds (DNN) et de machine learning (ML). Les clients peuvent créer une application RAG en utilisant le microservice Nvidia NIM & Nemo et Milvus comme base de données vectorielle. Une fois que le conteneur NVIDIA Nemo est déployé avec succès pour la génération d'enrobage, le conteneur Milvus peut être déployé pour stocker ces embedding. Pour plus d'informations sur les bases de données vectorielles et NetApp, voir https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-solution-with-netapp.html["Architecture de référence – solution Vector Database avec NetApp"^].



=== Apache Cassandra

Apache Cassandra®, une base de données NoSQL open source hautement évolutive et hautement disponible. Il est fourni avec des fonctions de recherche vectorielle et prend en charge les types de données vectorielles et les fonctions de recherche par similarité vectorielle, particulièrement utiles pour les applications d'IA impliquant des LMS et des pipelines RAG privés.

NetApp Instaclustr fournit un service entièrement géré pour Apache Cassandra®, hébergé dans le cloud ou sur site. Il permet aux clients NetApp de provisionner un cluster Apache Cassandra® et de se connecter au cluster à l'aide de C#, Node.js, AWS PrivateLink et de diverses autres options via la console Instaclustr ou l'API de provisionnement Instaclstr.

NetApp ONTAP sert également de fournisseur de stockage persistant pour les clusters conteneurisés Apache Cassandra s'exécutant sur Kubernetes. NetApp Astra Control étend de manière transparente les avantages de ONTAP en matière de gestion des données aux applications Kubernetes riches en données telles qu'Apache Cassandra. Pour plus d'informations, reportez-vous à la section https://cloud.netapp.com/hubfs/SB-4134-0321-DataStax-Cassandra-Guide%20(1).pdf["Gestion des données compatible avec les applications pour DataStax Enterprise avec NetApp Astra Control et le stockage ONTAP"^]



=== NetApp Instaclustr

Avec sa plateforme SaaS pour les technologies open source, Instaclustr aide les entreprises à mettre en place des applications à grande échelle et à prendre en charge leur infrastructure de données. Les développeurs d'IA génératifs qui souhaitent intégrer la compréhension sémantique dans leurs applications de recherche disposent d'une multitude d'options. Instaclustr pour Postgres prend en charge les extensions pgvector. Instaclustr pour OpenSearch prend en charge la recherche vectorielle pour récupérer les documents pertinents en fonction des requêtes d'entrée et des fonctions de voisinage le plus proche. Instaclustr pour Redis peut stocker des données vectorielles, récupérer des vecteurs et effectuer des recherches vectorielles. Pour plus d'informations, lisez https://www.instaclustr.com/platform/["Plateforme Instaclustr par NetApp"^]



== NetApp BlueXP

NetApp BlueXP unifie tous les services de stockage et de données NetApp dans un seul outil pour créer, protéger et gérer votre patrimoine de données multicloud hybride. Il offre une expérience unifiée pour le stockage et les services de données dans les environnements sur site et cloud, et favorise la simplicité opérationnelle grâce à la puissance des AIOps, avec les paramètres de consommation flexibles et la protection intégrée requis pour le monde d'aujourd'hui piloté par le cloud.



== NetApp Cloud Insights

NetApp Cloud Insights est un outil de surveillance de l'infrastructure cloud qui permet de bénéficier d'une grande visibilité sur l'ensemble de l'infrastructure. Avec Cloud Insights, vous pouvez surveiller toutes les ressources, les optimiser et résoudre les problèmes, y compris dans les clouds publics et dans vos data centers privés. Cloud Insights offre une visibilité complète de l'infrastructure et des applications à partir de centaines de collecteurs pour l'infrastructure et les workloads hétérogènes, y compris Kubernetes, le tout en un seul endroit. Pour plus d'informations, reportez-vous à la section https://docs.netapp.com/us-en/cloudinsights/index.html["Que peut m'apporter Cloud Insights ?"^]



== NetApp StorageGRID

NetApp StorageGRID est une suite de stockage objet Software-defined qui prend en charge un large éventail d'utilisations dans les environnements multiclouds publics, privés et hybrides. StorageGRID offre une prise en charge native de l'API Amazon S3 et propose des innovations de pointe, telles que la gestion automatisée du cycle de vie, pour stocker, sécuriser, protéger et conserver les données non structurées de manière économique sur de longues périodes.



== Spot NetApp

Spot by NetApp automatise et optimise votre infrastructure cloud sur AWS, Azure ou Google Cloud afin d'assurer une disponibilité et des performances conformes à vos SLA, et ce à moindre coût. Spot utilise des algorithmes de machine learning et d'analytique qui permettent de tirer parti de la capacité Spot pour les workloads stratégiques. Les clients qui exécutent des instances basées sur des processeurs graphiques peuvent tirer parti de Spot et réduire leurs coûts de calcul.



== NetApp ONTAP

ONTAP 9, la dernière génération de logiciel de gestion du stockage de NetApp, permet aux entreprises de moderniser l'infrastructure et de passer à un data Center prêt pour le cloud. Avec des capacités de gestion des données à la pointe du secteur, ONTAP permet de gérer et de protéger les données avec un seul ensemble d'outils, quel que soit leur emplacement. Vous pouvez aussi déplacer vos données librement partout où elles sont nécessaires : la périphérie, le cœur ou le cloud. ONTAP 9 comprend de nombreuses fonctionnalités qui simplifient la gestion des données, accélèrent et protègent les données stratégiques, et permettent d'utiliser des fonctionnalités d'infrastructure nouvelle génération dans toutes les architectures de cloud hybride.



=== Gestion simplifiée

La gestion des données est cruciale pour les opérations IT et les data Scientists, de sorte que les ressources appropriées sont utilisées pour les applications d'IA et pour l'entraînement des datasets d'IA/DE ML. Les informations supplémentaires suivantes sur les technologies NetApp ne sont pas incluses dans cette validation, mais elles peuvent être pertinentes en fonction de votre déploiement.

Le logiciel de gestion des données ONTAP comprend les fonctionnalités suivantes pour rationaliser et simplifier les opérations et réduire le coût total d'exploitation :

* Compaction des données à la volée et déduplication étendue La compaction des données réduit le gaspillage d'espace à l'intérieur des blocs de stockage, et la déduplication augmente considérablement la capacité effective. Cela s'applique aux données stockées localement et à leur placement dans le cloud.
* Qualité de service (AQoS) minimale, maximale et adaptative. Les contrôles granulaires de la qualité de service (QoS) permettent de maintenir les niveaux de performance des applications stratégiques dans des environnements hautement partagés.
* NetApp FabricPool Tiering automatique des données inactives vers des options de stockage de cloud public et privé, notamment Amazon Web Services (AWS), Azure et la solution de stockage NetApp StorageGRID. Pour plus d'informations sur FabricPool, voir https://www.netapp.com/pdf.html?item=/media/17239-tr4598pdf.pdf["Tr-4598 : meilleures pratiques de FabricPool"^].




=== Accélération et protection des données

ONTAP offre des niveaux supérieurs de performances et de protection des données et étend ces fonctionnalités aux méthodes suivantes :

* Des performances élevées et une faible latence. ONTAP offre le débit le plus élevé possible à la latence la plus faible possible.
* Protection des données. ONTAP fournit des fonctionnalités de protection des données intégrées avec une gestion commune sur toutes les plateformes.
* NetApp Volume Encryption (NVE). ONTAP offre un chiffrement natif au niveau du volume avec un support de gestion des clés interne et externe.
* Colocation et authentification multifacteur. ONTAP permet le partage des ressources d'infrastructure avec les plus hauts niveaux de sécurité.




=== Une infrastructure pérenne

ONTAP permet de répondre aux besoins métier en constante évolution grâce aux fonctionnalités suivantes :

* Évolutivité transparente et opérations non disruptives. ONTAP prend en charge l'ajout non disruptif de capacité aux contrôleurs et l'évolution scale-out des clusters. Les clients peuvent effectuer la mise à niveau vers les technologies les plus récentes, telles que NVMe et FC 32 Gb, sans migration des données ni panne coûteuse.
* Connexion cloud. ONTAP est le logiciel de gestion de stockage le plus connecté au cloud, avec des options de stockage Software-defined et les instances cloud natives dans tous les clouds publics.
* Intégration avec les applications émergentes ONTAP propose des services de données d'entreprise pour les plateformes et applications nouvelle génération, telles que les véhicules autonomes, les Smart cities et Industry 4.0, en utilisant la même infrastructure prenant en charge les applications d'entreprise existantes.




== Amazon FSX pour NetApp ONTAP

Amazon FSX pour NetApp ONTAP est un service AWS propriétaire et entièrement géré qui offre un stockage de fichiers extrêmement fiable, évolutif, haute performance et riche en fonctionnalités, basé sur le système de fichiers ONTAP populaire de NetApp. FSX for ONTAP associe les fonctionnalités, performances, capacités et opérations d'API connues des systèmes de fichiers NetApp, ainsi que l'agilité, l'évolutivité et la simplicité d'un service AWS entièrement géré.



== Azure NetApp Files

Azure NetApp Files est un service de stockage de fichiers Azure natif, propriétaire, haute performance. Il prend en charge les volumes SMB, NFS et à double protocole. Il peut également être utilisé dans les cas d'utilisation suivants :

* Partage de fichiers.
* Répertoires locaux.
* Bases de données.
* Une informatique haute performance.
* IA générative.




== Google Cloud NetApp volumes

Google Cloud NetApp volumes est un service de stockage des données cloud entièrement géré qui offre des fonctionnalités avancées de gestion des données et des performances hautement évolutives. Les données hébergées par NetApp peuvent être utilisées dans des opérations RAG (génération augmentée de récupération) pour la plateforme Vertex ai de Google, dans une architecture de référence avec kit d'outils prévisualisé.



== NetApp Astra Trident

ASTRA Trident permet la consommation et la gestion des ressources de stockage sur toutes les plateformes de stockage NetApp populaires, dans le cloud public ou sur site, y compris ONTAP (AFF, FAS, Select, Cloud, Amazon FSX pour NetApp ONTAP), Element (NetApp HCI, SolidFire), Azure NetApp Files service et Cloud Volumes Service sur Google Cloud. ASTRA Trident est un orchestrateur de stockage dynamique conforme à CSI (Container Storage interface) qui s'intègre de manière native à Kubernetes.



== Kubernetes

Kubernetes est une plateforme open source d'orchestration de conteneurs distribuée, conçue à l'origine par Google, et désormais gérée par Cloud Native Computing Foundation (CNCF). Kubernetes permet l'automatisation des fonctions de déploiement, de gestion et d'évolutivité pour les applications conteneurisées. En outre, il s'agit de la plateforme principale d'orchestration de conteneurs dans les environnements d'entreprise.
