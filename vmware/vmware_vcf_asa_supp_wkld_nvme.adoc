---
sidebar: sidebar 
permalink: vmware/vmware_vcf_asa_supp_wkld_nvme.html 
keywords: netapp, vmware, cloud, foundation, vcf, aff, all-flash, nfs, vvol, vvols, array, ontap tools, otv, sddc, iscsi 
summary:  
---
= Configuration du stockage supplémentaire NVMe/TCP pour les domaines de charge de travail VCF
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
Dans ce scénario, nous montrerons comment configurer du stockage supplémentaire NVMe/TCP pour un domaine de charge de travail VCF.

Auteur: Josh Powell



== Configuration du stockage supplémentaire NVMe/TCP pour les domaines de charge de travail VCF



=== Présentation du scénario

Ce scénario couvre les étapes générales suivantes :

* Créez une machine virtuelle de stockage (SVM) avec des interfaces logiques (LIF) pour le trafic NVMe/TCP.
* Créez des groupes de ports distribués pour les réseaux iSCSI sur le domaine de la charge de travail VI.
* Créez des adaptateurs vmkernel pour iSCSI sur les hôtes ESXi pour le domaine de charge de travail VI.
* Ajout d'adaptateurs NVMe/TCP sur les hôtes ESXi
* Déployez un datastore NVMe/TCP.




=== Prérequis

Ce scénario nécessite les composants et configurations suivants :

* Un système de stockage ONTAP ASA doté de ports de données physiques sur des commutateurs ethernet dédiés au trafic de stockage.
* Le déploiement du domaine de gestion VCF est terminé et le client vSphere est accessible.
* Un domaine de charge de travail VI a déjà été déployé.


NetApp recommande des designs réseau entièrement redondants pour NVMe/TCP. Le schéma suivant illustre un exemple de configuration redondante, fournissant une tolérance aux pannes pour les systèmes de stockage, les commutateurs, les cartes réseau et les systèmes hôtes. Reportez-vous au NetApp link:https://docs.netapp.com/us-en/ontap/san-config/index.html["Référence de configuration SAN"] pour plus d'informations.

image::vmware-vcf-asa-image74.png[Conception du réseau NVMe-tcp]

Pour les chemins d'accès multiples et le basculement sur plusieurs chemins, NetApp recommande de disposer d'au moins deux LIF par nœud de stockage dans des réseaux ethernet distincts pour tous les SVM dans des configurations NVMe/TCP.

Cette documentation explique le processus de création d'un SVM et de spécification des informations d'adresse IP pour créer plusieurs LIFs pour le trafic NVMe/TCP. Pour ajouter de nouvelles LIFs à un SVM existant, voir link:https://docs.netapp.com/us-en/ontap/networking/create_a_lif.html["Créer une LIF (interface réseau)"].

Pour plus d'informations sur les considérations relatives à la conception NVMe des systèmes de stockage ONTAP, reportez-vous à la section link:https://docs.netapp.com/us-en/ontap/nvme/support-limitations.html["Configuration, prise en charge et limitations de NVMe"].



=== Étapes de déploiement

Pour créer un datastore VMFS sur un domaine de charge de travail VCF à l'aide de NVMe/TCP, procédez comme suit.



==== Créez un SVM, des LIF et un namespace NVMe sur un système de stockage ONTAP

L'étape suivante s'effectue dans ONTAP System Manager.

.Créez la VM de stockage et les LIF
[%collapsible]
====
Effectuez les étapes suivantes pour créer un SVM avec plusieurs LIF pour le trafic NVMe/TCP.

. Dans le Gestionnaire système ONTAP, accédez à *Storage VMs* dans le menu de gauche et cliquez sur *+ Add* pour démarrer.
+
image::vmware-vcf-asa-image01.png[Cliquer sur +Ajouter pour commencer à créer une SVM]

+
{nbsp}

. Dans l'assistant *Add Storage VM*, indiquez un *Name* pour le SVM, sélectionnez *IP Space*, puis, sous *Access Protocol*, cliquez sur l'onglet *NVMe* et cochez la case *Enable NVMe/TCP*.
+
image::vmware-vcf-asa-image75.png[Assistant Add Storage VM : activez NVMe/TCP]

+
{nbsp}

. Dans la section *interface réseau*, remplissez les champs *adresse IP*, *masque de sous-réseau* et *domaine de diffusion et Port* pour la première LIF. Pour les LIF suivantes, la case à cocher peut être activée pour utiliser des paramètres communs à toutes les LIF restantes ou pour utiliser des paramètres distincts.
+

NOTE: Pour les chemins d'accès multiples et le basculement sur plusieurs chemins, NetApp recommande de disposer d'au moins deux LIF par nœud de stockage dans des réseaux Ethernet distincts pour tous les SVM dans des configurations NVMe/TCP.

+
image::vmware-vcf-asa-image76.png[Renseignez les informations réseau des LIF]

+
{nbsp}

. Indiquez si vous souhaitez activer le compte Storage VM Administration (pour les environnements en colocation) et cliquez sur *Save* pour créer le SVM.
+
image::vmware-vcf-asa-image04.png[Activer le compte SVM et Terminer]



====
.Créez le namespace NVMe
[%collapsible]
====
Les espaces de noms NVMe sont analogues aux LUN pour iSCSI ou FC. L'espace de noms NVMe doit être créé avant de pouvoir déployer un datastore VMFS à partir du client vSphere. Pour créer l'espace de noms NVMe, vous devez d'abord obtenir le nom NQN (NVMe Qualified Name) de chaque hôte ESXi du cluster. Le NQN est utilisé par ONTAP pour fournir un contrôle d'accès à l'espace de noms.

Pour créer un namespace NVMe, procédez comme suit :

. Ouvrez une session SSH avec un hôte ESXi dans le cluster pour obtenir son NQN. Utiliser la commande suivante depuis l'interface de ligne de commande :
+
[source, cli]
----
esxcli nvme info get
----
+
Une sortie similaire à la suivante doit s'afficher :

+
[source, cli]
----
Host NQN: nqn.2014-08.com.netapp.sddc:nvme:vcf-wkld-esx01
----
. Enregistrez le NQN pour chaque hôte ESXi du cluster
. Dans le Gestionnaire système ONTAP, naviguez jusqu'à *Namespaces NVMe* dans le menu de gauche et cliquez sur *+ Add* pour démarrer.
+
image::vmware-vcf-asa-image93.png[Cliquez sur +Ajouter pour créer l'espace de noms NVMe]

+
{nbsp}

. Sur la page *Ajouter un espace de noms NVMe*, indiquez un préfixe de nom, le nombre d'espaces de noms à créer, la taille de l'espace de noms et le système d'exploitation hôte qui accédera à l'espace de noms. Dans la section *Host NQN*, créez une liste séparée par des virgules des NQN précédemment collectés auprès des hôtes ESXi qui accéderont aux espaces de noms.


Cliquez sur *plus d'options* pour configurer des éléments supplémentaires tels que la stratégie de protection des snapshots. Enfin, cliquez sur *Save* pour créer l'espace de noms NVMe.

+ image::vmware-vcf-asa-image93.png[cliquez sur +Ajouter pour créer un espace de noms NVMe]

====


==== Configuration du réseau et des adaptateurs logiciels NVMe sur des hôtes ESXi

Les étapes suivantes sont effectuées sur le cluster du domaine de la charge de travail VI à l'aide du client vSphere. Dans ce cas, l'authentification unique vCenter est utilisée, de sorte que le client vSphere est commun aux domaines de la gestion et de la charge de travail.

.Créez des groupes de ports distribués pour le trafic NVME/TCP
[%collapsible]
====
Pour créer un nouveau groupe de ports distribués pour chaque réseau NVMe/TCP, procédez comme suit :

. Dans le client vSphere , accédez à *Inventory > Networking* pour le domaine de charge de travail. Naviguez jusqu'au commutateur distribué existant et choisissez l'action pour créer *Nouveau groupe de ports distribués...*.
+
image::vmware-vcf-asa-image22.png[Choisissez de créer un nouveau groupe de ports]

+
{nbsp}

. Dans l'assistant *Nouveau groupe de ports distribués*, entrez un nom pour le nouveau groupe de ports et cliquez sur *Suivant* pour continuer.
. Sur la page *configurer les paramètres*, remplissez tous les paramètres. Si des VLAN sont utilisés, assurez-vous de fournir l'ID de VLAN correct. Cliquez sur *Suivant* pour continuer.
+
image::vmware-vcf-asa-image23.png[Remplir l'ID VLAN]

+
{nbsp}

. Sur la page *prêt à terminer*, passez en revue les modifications et cliquez sur *Terminer* pour créer le nouveau groupe de ports distribués.
. Répétez ce processus pour créer un groupe de ports distribués pour le deuxième réseau NVMe/TCP utilisé et assurez-vous que vous avez entré l'ID *VLAN* correct.
. Une fois les deux groupes de ports créés, naviguez jusqu'au premier groupe de ports et sélectionnez l'action *Modifier les paramètres...*.
+
image::vmware-vcf-asa-image77.png[DPG - permet de modifier les paramètres]

+
{nbsp}

. Sur la page *Distributed Port Group - Edit Settings*, accédez à *Teaming and failover* dans le menu de gauche et cliquez sur *uplink2* pour le déplacer vers *uplinks* inutilisés.
+
image::vmware-vcf-asa-image78.png[déplacez uplink2 vers inutilisé]

. Répétez cette étape pour le deuxième groupe de ports NVMe/TCP. Cependant, cette fois, déplacez *uplink1* vers *uplinks* inutilisés.
+
image::vmware-vcf-asa-image79.png[déplacer la liaison montante 1 vers inutilisé]



====
.Créez des adaptateurs VMkernel sur chaque hôte ESXi
[%collapsible]
====
Répétez ce processus sur chaque hôte ESXi du domaine de charge de travail.

. À partir du client vSphere, accédez à l'un des hôtes ESXi de l'inventaire du domaine de charge de travail. Dans l'onglet *configurer*, sélectionnez *adaptateurs VMkernel* et cliquez sur *Ajouter réseau...* pour démarrer.
+
image::vmware-vcf-asa-image30.png[Démarrez l'assistant d'ajout de réseau]

+
{nbsp}

. Dans la fenêtre *Select connection type*, choisissez *VMkernel Network adapter* et cliquez sur *Next* pour continuer.
+
image::vmware-vcf-asa-image08.png[Choisissez VMkernel Network adapter]

+
{nbsp}

. Sur la page *Sélectionner le périphérique cible*, choisissez l'un des groupes de ports distribués pour iSCSI créés précédemment.
+
image::vmware-vcf-asa-image95.png[Choisissez le groupe de ports cible]

+
{nbsp}

. Sur la page *Port properties*, cliquez sur la case *NVMe over TCP* et cliquez sur *Next* pour continuer.
+
image::vmware-vcf-asa-image96.png[Propriétés du port VMkernel]

+
{nbsp}

. Sur la page *IPv4 settings*, remplissez *adresse IP*, *masque de sous-réseau* et fournissez une nouvelle adresse IP de passerelle (uniquement si nécessaire). Cliquez sur *Suivant* pour continuer.
+
image::vmware-vcf-asa-image97.png[Paramètres IPv4 VMkernel]

+
{nbsp}

. Consultez vos sélections sur la page *prêt à terminer* et cliquez sur *Terminer* pour créer l'adaptateur VMkernel.
+
image::vmware-vcf-asa-image98.png[Vérifiez les sélections VMkernel]

+
{nbsp}

. Répétez cette procédure pour créer un adaptateur VMkernel pour le second réseau iSCSI.


====
.Ajout de l'adaptateur NVMe over TCP
[%collapsible]
====
Chaque hôte ESXi du cluster de domaine de charge de travail doit disposer d'un adaptateur logiciel NVMe over TCP installé pour chaque réseau NVMe/TCP établi dédié au trafic de stockage.

Pour installer les adaptateurs NVMe over TCP et découvrir les contrôleurs NVMe, effectuez les opérations suivantes :

. Dans le client vSphere, accédez à l'un des hôtes ESXi du cluster du domaine de charge de travail. Dans l'onglet *Configure*, cliquez sur *Storage Adapters* dans le menu, puis, dans le menu déroulant *Add Software adapter*, sélectionnez *Add NVMe over TCP adapter*.
+
image::vmware-vcf-asa-image99.png[Ajout de l'adaptateur NVMe over TCP]

+
{nbsp}

. Dans la fenêtre *Add Software NVMe over TCP adapter*, accédez au menu déroulant *Physical Network adapter* et sélectionnez l'adaptateur réseau physique approprié sur lequel activer l'adaptateur NVMe.
+
image::vmware-vcf-asa-image100.png[Sélectionnez une carte physique]

+
{nbsp}

. Répétez cette procédure pour le second réseau attribué au trafic NVMe sur TCP, en attribuant l'adaptateur physique approprié.
. Sélectionnez l'un des adaptateurs NVMe over TCP récemment installés et, dans l'onglet *contrôleurs*, sélectionnez *Ajouter un contrôleur*.
+
image::vmware-vcf-asa-image101.png[Ajouter un contrôleur]

+
{nbsp}

. Dans la fenêtre *Ajouter contrôleur*, sélectionnez l'onglet *automatiquement* et procédez comme suit.
+
** Remplissez les adresses IP de l'une des interfaces logiques du SVM sur le même réseau que l'adaptateur physique affecté à cet adaptateur NVMe over TCP.
** Cliquez sur le bouton *détecter contrôleurs*.
** Dans la liste des contrôleurs découverts, cochez la case des deux contrôleurs dont les adresses réseau sont alignées sur cet adaptateur NVMe over TCP.
** Cliquez sur le bouton *OK* pour ajouter les contrôleurs sélectionnés.
+
image::vmware-vcf-asa-image102.png[Détection et ajout de contrôleurs]

+
{nbsp}



. Au bout de quelques secondes, l'espace de nom NVMe s'affiche dans l'onglet Devices.
+
image::vmware-vcf-asa-image103.png[Espace de noms NVMe répertorié sous Devices]

+
{nbsp}

. Répétez cette procédure pour créer un adaptateur NVMe over TCP pour le second réseau établi pour le trafic NVMe/TCP.


====
.Déployez le datastore NVMe over TCP
[%collapsible]
====
Pour créer un datastore VMFS sur l'espace de noms NVMe, effectuez les opérations suivantes :

. Dans le client vSphere, accédez à l'un des hôtes ESXi du cluster du domaine de charge de travail. Dans le menu *actions*, sélectionnez *stockage > Nouveau datastore...*.
+
image::vmware-vcf-asa-image104.png[Ajout de l'adaptateur NVMe over TCP]

+
{nbsp}

. Dans l'assistant *Nouveau datastore*, sélectionnez *VMFS* comme type. Cliquez sur *Suivant* pour continuer.
. Sur la page *sélection du nom et du périphérique*, indiquez un nom pour le datastore et sélectionnez l'espace de noms NVMe dans la liste des périphériques disponibles.
+
image::vmware-vcf-asa-image105.png[Sélection du nom et du périphérique]

+
{nbsp}

. Sur la page *VMFS version*, sélectionnez la version de VMFS pour le datastore.
. Sur la page *partition configuration*, apportez les modifications souhaitées au schéma de partition par défaut. Cliquez sur *Suivant* pour continuer.
+
image::vmware-vcf-asa-image106.png[Configuration des partitions NVMe]

+
{nbsp}

. Sur la page *prêt à terminer*, passez en revue le résumé et cliquez sur *Terminer* pour créer le datastore.
. Naviguez jusqu'au nouveau datastore de l'inventaire et cliquez sur l'onglet *hosts*. S'il est configuré correctement, tous les hôtes ESXi du cluster doivent être répertoriés et avoir accès au nouveau datastore.
+
image::vmware-vcf-asa-image107.png[Hôtes connectés au datastore]

+
{nbsp}



====


=== Informations supplémentaires

Pour plus d'informations sur la configuration des systèmes de stockage ONTAP, reportez-vous au link:https://docs.netapp.com/us-en/ontap["Documentation ONTAP 9"] centre.

Pour plus d'informations sur la configuration de VCF, reportez-vous à la section link:https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html["Documentation de VMware Cloud Foundation"].
